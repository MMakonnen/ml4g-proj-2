{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES FOR ATREYA\n",
    "# - careful, for train test split of single cell data patient 7 is the only person with 2 samples hence make sure to not use one sample from this person for \n",
    "# train and one for test -> will lead to overly optimistic results\n",
    "# NOTE: we have labels BUT DO NOT TAKE supervised regression approach, the labels are only for cluster performance assesment\n",
    "# - careful when submitting and zipping mac creates the MACOX files in the zipped file !!!! -> this will not comply with the required\n",
    "# submission format and hence lose us a submission, make sure you check this !!!\n",
    "# - they provided some sanity checks in the reference code that is meant as a starting point, check those (listed directly below here for the clustering task)\n",
    "# - zip everything up properly as described in the \n",
    "# - change my hardcoded data paths (harcoded since at some point the relatice paths started acting up)\n",
    "# - use my zip file with the data since that one contains the data in a better structured format\n",
    "# - dont forget the requirements file, also here some notes with respect to the installs:\n",
    "# use python 3.10, numpy version 1.6.24 (latest one before numpy 2....), and then conda pip or fork install all the required libraries as you go \n",
    "# (you will see when running the cells)\n",
    "# - if time extend the comments and fix the code a bit (but low prio)\n",
    "# - remove this section\n",
    "# - combine this with the other script to be able to zip up all the results of the two in one (see reference code)\n",
    "\n",
    "# THINGS TO PLAY AROUND WITH AND IMPROVE\n",
    "# - all the hyperparameters (e.g. number of princicapl components for PCA, cutoffs for outliers, ...)\n",
    "# - another batching approach (e.g. combat instead of harmony), another clustering approach or new combinations of these\n",
    "# -> recall that certain batching approaches work better with certain clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of cells in the train set \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_adata\u001b[49m\u001b[38;5;241m.\u001b[39mn_obs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spl \u001b[38;5;129;01min\u001b[39;00m train_adata\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mSample\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of cells for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_adata[train_adata\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mSample\u001b[38;5;241m==\u001b[39mspl]\u001b[38;5;241m.\u001b[39mn_obs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_adata' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Number of cells in the train set {train_adata.n_obs}\")\n",
    "for spl in train_adata.obs.Sample.unique():\n",
    "    print(f\"Number of cells for {spl} is {train_adata[train_adata.obs.Sample==spl].n_obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {train_adata.obs.highLevelType.nunique()} different cell types in the dataset\")\n",
    "print(f\"The different cells types are {train_adata.obs.highLevelType.unique().astype(str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of cells in the test set {test_adata.n_obs}\")\n",
    "for spl in test_adata.obs.Sample.unique():\n",
    "    print(f\"Number of cells for {spl} is {test_adata[test_adata.obs.Sample==spl].n_obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/anndata/utils.py:429: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scanpy.external as sce\n",
    "\n",
    "# reproducibility\n",
    "seed = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients in training data: ['P1', 'P2', 'P3', 'P4', 'P7']\n",
      "Categories (5, object): ['P1', 'P2', 'P3', 'P4', 'P7']\n",
      "Training patients: ['P2', 'P7', 'P1']\n",
      "Categories (5, object): ['P1', 'P2', 'P3', 'P4', 'P7']\n",
      "Validation patients: ['P4', 'P3']\n",
      "Categories (5, object): ['P1', 'P2', 'P3', 'P4', 'P7']\n"
     ]
    }
   ],
   "source": [
    "# split training data to prevent leakage (since patient 7 two samples)\n",
    "\n",
    "# Load training data\n",
    "train_adata_path = '/Users/mikael/Documents/ETHZ/Courses/S3_Autumn_2024/ML_Genomics/projects/project_2/ml4g-proj-2/data/cluster/train/train_adata.h5ad'\n",
    "train_adata = sc.read_h5ad(train_adata_path)\n",
    "\n",
    "# List of unique patients\n",
    "patients = train_adata.obs['Patient'].unique()\n",
    "print(\"Patients in training data:\", patients)\n",
    "\n",
    "# Split patients into training and validation sets\n",
    "train_patients, val_patients = train_test_split(\n",
    "    patients, test_size=0.3, random_state=seed\n",
    ")\n",
    "\n",
    "# Subset the data\n",
    "train_subset = train_adata[train_adata.obs['Patient'].isin(train_patients)].copy()\n",
    "validation_subset = train_adata[train_adata.obs['Patient'].isin(val_patients)].copy()\n",
    "\n",
    "print(\"Training patients:\", train_patients)\n",
    "print(\"Validation patients:\", val_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/l4sw2ybd7t3b9gbsrzngsn6w0000gn/T/ipykernel_40333/3985071899.py:2: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  combined_train_adata = train_subset.concatenate(\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation subsets to ensure consistent preprocessing and batch correction\n",
    "combined_train_adata = train_subset.concatenate(\n",
    "    validation_subset,\n",
    "    batch_key='validation_split',\n",
    "    batch_categories=['train', 'validation'],\n",
    "    index_unique=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:165: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[\"n_counts\"] = number\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:75: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
      "  warnings.warn(\n",
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/scanpy/preprocessing/_scale.py:318: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "2024-12-03 23:35:43,088 - harmonypy - INFO - Computing initial centroids with sklearn.KMeans...\n",
      "2024-12-03 23:35:46,495 - harmonypy - INFO - sklearn.KMeans initialization complete.\n",
      "2024-12-03 23:35:46,537 - harmonypy - INFO - Iteration 1 of 10\n",
      "2024-12-03 23:35:48,870 - harmonypy - INFO - Iteration 2 of 10\n",
      "2024-12-03 23:35:51,962 - harmonypy - INFO - Iteration 3 of 10\n",
      "2024-12-03 23:35:54,242 - harmonypy - INFO - Iteration 4 of 10\n",
      "2024-12-03 23:35:56,952 - harmonypy - INFO - Iteration 5 of 10\n",
      "2024-12-03 23:35:59,037 - harmonypy - INFO - Converged after 5 iterations\n"
     ]
    }
   ],
   "source": [
    "# preprocess combined data\n",
    "# - Mitochondrial gene expression is used as a quality metric because high levels may indicate cell stress or death\n",
    "# - Cells with high mitochondrial content (>5%) are filtered out\n",
    "# - Cells with extremely high total counts or gene numbers are also filtered to remove potential doublets (two cells captured as one)\n",
    "# - Normalization adjusts for differences in sequencing depth across cells by scaling each cell's total counts to a common value (10,000)\n",
    "# - Log Transformation stabilizes the variance across genes, making the data more suitable for downstream analysis\n",
    "\n",
    "\n",
    "# quality control\n",
    "\n",
    "\n",
    "# Identify mitochondrial genes\n",
    "combined_train_adata.var['mt'] = combined_train_adata.var_names.str.startswith('MT-')\n",
    "\n",
    "# Calculate QC metrics\n",
    "sc.pp.calculate_qc_metrics(\n",
    "    combined_train_adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True\n",
    ")\n",
    "\n",
    "# Filter cells\n",
    "combined_train_adata = combined_train_adata[combined_train_adata.obs.pct_counts_mt < 5, :]\n",
    "sc.pp.filter_cells(combined_train_adata, max_counts=25000)\n",
    "sc.pp.filter_cells(combined_train_adata, max_genes=6000)\n",
    "\n",
    "\n",
    "# Normalization and Log Transformation\n",
    "\n",
    "\n",
    "# Normalize total counts per cell\n",
    "sc.pp.normalize_total(combined_train_adata, target_sum=1e4)\n",
    "\n",
    "# Logarithmize the data\n",
    "sc.pp.log1p(combined_train_adata)\n",
    "\n",
    "\n",
    "# Highly Variable Genes (HVGs)\n",
    "\n",
    "# Identify Highly Variable Genes (HVGs) using Seurat's method while accounting for batch effects\n",
    "# NOTE: Highly Variable Genes are selected because they provide the most information for distinguishing between different cell types\n",
    "\n",
    "\n",
    "# Identify HVGs\n",
    "sc.pp.highly_variable_genes(\n",
    "    combined_train_adata,\n",
    "    flavor='seurat_v3',\n",
    "    n_top_genes=2000,\n",
    "    batch_key='Sample'\n",
    ")\n",
    "\n",
    "# Subset to HVGs\n",
    "combined_train_adata = combined_train_adata[:, combined_train_adata.var.highly_variable]\n",
    "\n",
    "\n",
    "# Scaling and PCA\n",
    "# Perform PCA to reduce dimensionality\n",
    "# WHY?: PCA simplifies scRNA-seq data by extracting key components, improving signal-to-noise ratio, \n",
    "# enabling effective batch correction and focusing clustering on biologically meaningful differences\n",
    "# leading to more robustness of insights\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "sc.pp.scale(combined_train_adata, max_value=10)\n",
    "\n",
    "# Perform PCA\n",
    "sc.tl.pca(combined_train_adata, svd_solver='arpack', n_comps=50)\n",
    "\n",
    "\n",
    "# Batch Correction with Harmony\n",
    "# NOTE: Harmony adjusts for batch effects in the PCA embeddings, preserving biological variability\n",
    "\n",
    "\n",
    "# Run Harmony integration on PCA embeddings\n",
    "sce.pp.harmony_integrate(combined_train_adata, key='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/miniconda3/envs/ml4g-proj-2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/tp/l4sw2ybd7t3b9gbsrzngsn6w0000gn/T/ipykernel_40333/1051185770.py:11: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n",
      "\n",
      " To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n",
      "  sc.tl.leiden(combined_train_adata, resolution=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Clustering on combined data\n",
    "# - A k-Nearest Neighbors (KNN) graph is constructed using the Harmony-corrected PCA embeddings\n",
    "# - graph represents the similarity between cells based on their gene expression profiles\n",
    "# - Leiden Clustering detects communities (clusters) in the KNN graph\n",
    "\n",
    "\n",
    "# Compute neighborhood graph\n",
    "sc.pp.neighbors(combined_train_adata, n_neighbors=10, use_rep='X_pca_harmony')\n",
    "\n",
    "# Cluster using Leiden algorithm\n",
    "sc.tl.leiden(combined_train_adata, resolution=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Adjusted Rand Index (ARI): 0.3603\n",
      "Validation V-measure Score: 0.6294\n",
      "Validation Clustering Score: 0.4949\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Clustering Performance on Validation Set\n",
    "\n",
    "# Split combined data back\n",
    "train_indices = combined_train_adata.obs['validation_split'] == 'train'\n",
    "val_indices = combined_train_adata.obs['validation_split'] == 'validation'\n",
    "\n",
    "train_subset = combined_train_adata[train_indices].copy()\n",
    "validation_subset = combined_train_adata[val_indices].copy()\n",
    "\n",
    "\n",
    "# eval cluster performance\n",
    "\n",
    "# True labels and predicted clusters for validation data\n",
    "true_labels = validation_subset.obs['highLevelType']\n",
    "pred_labels = validation_subset.obs['leiden']\n",
    "\n",
    "# Compute ARI\n",
    "ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "print(f\"Validation Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "\n",
    "# Compute V-measure\n",
    "v_measure = v_measure_score(true_labels, pred_labels)\n",
    "print(f\"Validation V-measure Score: {v_measure:.4f}\")\n",
    "\n",
    "# Combined metric\n",
    "clustering_score = 0.5 * v_measure + 0.5 * ari\n",
    "print(f\"Validation Clustering Score: {clustering_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCEED WITH TEST DATA CLUSTERING\n",
    "\n",
    "# Load test data\n",
    "test_adata = sc.read_h5ad('test_adata.h5ad')\n",
    "\n",
    "# Combine training data and test data\n",
    "combined_adata = train_adata.concatenate(\n",
    "    test_adata,\n",
    "    batch_key='dataset',\n",
    "    batch_categories=['train', 'test'],\n",
    "    index_unique=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat preprocessing steps\n",
    "\n",
    "# Quality control\n",
    "combined_adata.var['mt'] = combined_adata.var_names.str.startswith('MT-')\n",
    "sc.pp.calculate_qc_metrics(\n",
    "    combined_adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True\n",
    ")\n",
    "combined_adata = combined_adata[combined_adata.obs.pct_counts_mt < 5, :]\n",
    "sc.pp.filter_cells(combined_adata, max_counts=25000)\n",
    "sc.pp.filter_cells(combined_adata, max_genes=6000)\n",
    "\n",
    "# Normalization and log transformation\n",
    "sc.pp.normalize_total(combined_adata, target_sum=1e4)\n",
    "sc.pp.log1p(combined_adata)\n",
    "\n",
    "# HVGs\n",
    "sc.pp.highly_variable_genes(\n",
    "    combined_adata,\n",
    "    flavor='seurat_v3',\n",
    "    n_top_genes=2000,\n",
    "    batch_key='Sample'\n",
    ")\n",
    "combined_adata = combined_adata[:, combined_adata.var.highly_variable]\n",
    "\n",
    "# Scaling and PCA\n",
    "sc.pp.scale(combined_adata, max_value=10)\n",
    "sc.tl.pca(combined_adata, svd_solver='arpack', n_comps=50)\n",
    "\n",
    "# Harmony integration\n",
    "sce.pp.harmony_integrate(combined_adata, key='Sample')\n",
    "\n",
    "# Clustering\n",
    "sc.pp.neighbors(combined_adata, n_neighbors=10, use_rep='X_pca_harmony')\n",
    "sc.tl.leiden(combined_adata, resolution=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cluster assignments for test data\n",
    "\n",
    "# CAREFUL WITH THE FORMATING\n",
    "# NOTE TOO SURE IF BELOW WORKS FINE\n",
    "\n",
    "# # Separate test data\n",
    "# test_indices = combined_adata.obs['dataset'] == 'test'\n",
    "# test_adata = combined_adata[test_indices].copy()\n",
    "\n",
    "# # Prepare cluster membership DataFrame\n",
    "# cluster_membership = pd.DataFrame({\n",
    "#     'index': np.arange(len(test_adata)),\n",
    "#     'cluster': test_adata.obs['leiden'].astype(int) + 1  # Ensure 1-indexed clusters\n",
    "# })\n",
    "\n",
    "# # Save to CSV\n",
    "# cluster_membership.to_csv('cluster_membership.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g-proj-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
